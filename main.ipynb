{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbcaff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "def warn(*args, **kwargs): pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e220d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c75b3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch, random, numpy as np, matplotlib.pyplot as plt, os, pickle, pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43adb48e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.system('nvidia-smi -q -d Memory |grep -A6 GPU|grep Free >tmp')\n",
    "memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "gpu_number =  int(np.argmax(memory_available))\n",
    "torch.cuda.set_device(gpu_number)\n",
    "print(f\"Cuda:{gpu_number}\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set dataset and parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_type = \"DailySports\" #RotatedMNIST, FEMNIST, CIFAR10C, DailySports, WISDM\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(1, \"./\" + dataset_type)\n",
    "import data, params, models\n",
    "from utils import write_in_file, plot_avg_acc, plot_worst_acc\n",
    "from trainingUMA import UMA, adapt_evaluate_dann, adapt_evaluate_mmd, eval_domains\n",
    "from trainingARM import ARM_CML, train, test\n",
    "\n",
    "# For reproducibility\n",
    "torch.random.manual_seed(params.seed)\n",
    "random.seed(params.seed)\n",
    "np.random.seed(params.seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for it in range(3):\n",
    "    clear_output(wait=True)\n",
    "    PATH = \"./\" + dataset_type + \"/Results\" + str(it+1) + \"/\"\n",
    "    if not os.path.exists(PATH): os.makedirs(PATH)\n",
    "    print(\"DATASET\") \n",
    "    if dataset_type == \"RotatedMNIST\":\n",
    "        domains_train_idxs = random.sample(range(params.nb_domains_tot), params.nb_domains_train)\n",
    "        domains_test_idxs = list(set(range(params.nb_domains_tot)) - set(domains_train_idxs))\n",
    "        #domains_train_idxs = pd.read_pickle(PATH+\"domains_train\")\n",
    "        #domains_test_idxs = pd.read_pickle(PATH+\"domains_test\")\n",
    "        rotatedMnist = data.RotatedMNIST(print_output=False)\n",
    "        domains_train, domains_test = data.domain_generator(rotatedMnist, domains_train_idxs, domains_test_idxs)\n",
    "        dataset_train = data.RotatedMNIST(domains_idxs = domains_train_idxs, split='train', print_output=False)\n",
    "        dataset_test = data.RotatedMNIST(domains_idxs = domains_test_idxs, split='test', print_output=False)    \n",
    "        write_in_file(domains_train_idxs, PATH+\"domains_train\")\n",
    "        write_in_file(domains_test_idxs, PATH+\"domains_test\")\n",
    "    elif dataset_type == \"FEMNIST\":\n",
    "        dataset_train = data.FEMNISTDataset(split=\"train\")\n",
    "        domains_train = data.domain_generator(dataset_train, split=\"train\")\n",
    "        dataset_test = data.FEMNISTDataset(split=\"test\")\n",
    "        domains_test = data.domain_generator(dataset_test)\n",
    "    elif dataset_type == \"CIFAR10C\":\n",
    "        dataset_train = data.CIFARDataset(split='train')\n",
    "        domains_train = data.domain_generator(dataset_train)\n",
    "        dataset_test = data.CIFARDataset(split='test')\n",
    "        domains_test = data.domain_generator(dataset_test, test='True')\n",
    "    elif dataset_type == \"DailySports\":\n",
    "        root_dir = \"DailySports/\"\n",
    "        #domains_train_idxs = random.sample(range(params.nb_domains_tot), params.nb_domains_train)\n",
    "        #domains_test_idxs = list(set(range(params.nb_domains_tot)) - set(domains_train_idxs))\n",
    "        domains_train_idxs = pd.read_pickle(PATH+\"domains_train\")\n",
    "        domains_test_idxs = pd.read_pickle(PATH+\"domains_test\")\n",
    "        HAR_Dataset = data.HARDataset(root_dir, feature_reduction=params.feature_reduction, extract_features=params.extract_features)\n",
    "        domains_train, domains_test = data.domain_generator(HAR_Dataset, domains_train_idxs, domains_test_idxs)\n",
    "        dataset_train = data.HARDataset(root_dir, domains_train_idxs, feature_reduction=params.feature_reduction, extract_features=params.extract_features)\n",
    "        dataset_test = data.HARDataset(root_dir, domains_test_idxs, feature_reduction=params.feature_reduction, extract_features=params.extract_features)\n",
    "    elif dataset_type == \"WISDM\":  \n",
    "        root_dir = \"WISDM/\"\n",
    "        domains_total = list(range(params.nb_domains_tot))\n",
    "        #Remove subjects with corrupted data\n",
    "        domains_total.remove(14)\n",
    "        domains_total.remove(18)\n",
    "        domains_total.remove(42)\n",
    "        domains_train_idxs = random.sample(domains_total, params.nb_domains_train)\n",
    "        domains_test_idxs = list(set(domains_total)-set(domains_train_idxs))\n",
    "        WISDM_Dataset = data.WISDMDataset(root_dir, domains_total, datatype = params.data_type, extract_features=params.extract_features)\n",
    "        domains_train, domains_test = data.domain_generator(WISDM_Dataset, domains_train_idxs, domains_test_idxs)\n",
    "        dataset_train = data.WISDMDataset(root_dir, domains_train_idxs, datatype=params.data_type, extract_features=params.extract_features)\n",
    "        dataset_test = data.WISDMDataset(root_dir, domains_test_idxs, datatype=params.data_type, extract_features=params.extract_features)\n",
    "        \n",
    "    train_loader = data.get_loader(dataset_train, sampler_type='group', uniform_over_groups=1,\n",
    "                              meta_batch_size=params.meta_batch_size,\n",
    "                              support_size=params.support_size,\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True, num_workers=8)\n",
    "    test_loader = data.get_loader(dataset_test, sampler_type='group', uniform_over_groups=False,\n",
    "                              meta_batch_size=params.meta_batch_size,\n",
    "                              support_size=params.support_size,\n",
    "                              shuffle=False,\n",
    "                              pin_memory=True, num_workers=8)  \n",
    "\n",
    "    print(\"MODELS-UMA\")\n",
    "    #UMA models\n",
    "    model_dann = models.DANNModel().to(DEVICE)\n",
    "    uma_dann = UMA(model_dann, params.loss.to(DEVICE), params.lr_sgd_cc, params.lr_sgd_dd, params.lr_adam, adapt_steps=params.ADAPT_STEPS,  model_type = 'dann', adaptive_lr=True, batch_norm=params.batch_norm).fit_dann(domains_train, alpha=params.alpha_, steps=params.training_steps)\n",
    "    torch.save(uma_dann, PATH+\"model_dann.pkl\")\n",
    "#    uma_dann = torch.load(PATH+\"model_dann.pkl\").to(DEVICE)\n",
    "\n",
    "    model_mmd = models.MMDModel().to(DEVICE)\n",
    "    uma_mmd = UMA(model_mmd, params.loss.to(DEVICE), params.lr_sgd_cc, params.lr_sgd_dd, params.lr_adam, adapt_steps=params.ADAPT_STEPS, model_type = 'mmd', adaptive_lr=True, sigma = params.sigma_, batch_norm=params.batch_norm).fit_mmd(domains_train, steps=params.training_steps)\n",
    "    torch.save(uma_mmd, PATH+\"model_mmd.pkl\")\n",
    "#    uma_mmd = torch.load(PATH+\"model_mmd.pkl\").to(DVEICE)\n",
    "    \n",
    "    print(\"MODELS-SCRATCH\")\n",
    "    #models from scratch\n",
    "    model_dann_scratch = models.DANNModel().to(DEVICE) \n",
    "    model_mmd_scratch = models.MMDModel().to(DEVICE)\n",
    "    \n",
    "    print(\"MODELS-ARM\")\n",
    "    # ARM CML\n",
    "    context_net = models.ContextNet(in_dim=params.input_dim, out_dim=params.output_dim, hidden_dim=params.hidden_dim_context).to(DEVICE)\n",
    "    model_arm = models.ARMNet(in_dim=params.input_dim_arm, hidden_dim=params.hidden_dim_net, n_classes=params.num_classes).to(DEVICE)\n",
    "    algorithm = ARM_CML(model_arm, context_net, params.input_dim, params.loss.to(DEVICE), learning_rate=params.learning_rate, weight_decay=params.weight_decay, optimizer=params.optimizer, momentum=params.momentum, support_size=params.support_size, device=DEVICE, adapt_bn=params.adapt_bn, img_dataset=params.img_dataset)\n",
    "    history_arm = train(algorithm, train_loader, test_loader, epochs = params.epochsARM)\n",
    "    torch.save(algorithm, PATH+\"model_arm.pkl\")\n",
    "#    algorithm = torch.load(PATH+\"model_arm.pkl\").to(DEVICE)\n",
    "    \n",
    "    print(\"PERFORMANCE\")\n",
    "    worst_acc_umaDANN, avg_acc_umaDANN, testacc_umaDANN = eval_domains(uma_dann.model, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_umaDANN\")\n",
    "    worst_acc_scratchDANN, avg_acc_scratchDANN, testacc_scratchDANN = eval_domains(model_dann_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_scratchDANN\")\n",
    "    worst_acc_umaMMD, avg_acc_umaMMD, testacc_umaMMD = eval_domains(uma_mmd.model, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_umaMMD\")\n",
    "    worst_acc_scratchMMD, avg_acc_scratchMMD, testacc_scratchMMD = eval_domains(model_mmd_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_scratchMMD\")\n",
    "\n",
    "    plot_avg_acc(params.test_steps, avg_acc_umaDANN, avg_acc_umaMMD, avg_acc_scratchDANN, avg_acc_scratchMMD, save_file=PATH+\"average_accuracy.png\")\n",
    "    plot_worst_acc(params.test_steps, worst_acc_umaDANN, worst_acc_umaMMD, worst_acc_scratchDANN, worst_acc_scratchMMD, save_file=PATH+\"worst_accuracy.png\")\n",
    "    \n",
    "    worst_case_acc, average_case_acc, stats = test(algorithm, test_loader, n_samples_per_group=params.n_samples_test, save_output=PATH+\"perf_ARM\")\n",
    " \n",
    "    print(\"#SAMPLES COMPARISON\")\n",
    "    n_samples = [50, 100, 145]\n",
    "    acc_umaDANN_samples, acc_umaMMD_samples, acc_scratchDANN_samples, acc_scratchMMD_samples, acc_ARMCML_samples=[], [], [], [], []\n",
    "    for s in n_samples:\n",
    "        print(s)\n",
    "        seed_ = random.randint(0, 100)\n",
    "        _, avg_acc_umaDANN_tmp, _ = eval_domains(uma_dann.model, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "        _, avg_acc_umaMMD_tmp, _ = eval_domains(uma_mmd.model, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "\n",
    "        _, avg_acc_scratchDANN_tmp, _ = eval_domains(model_dann_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "        _, avg_acc_scratchMMD_tmp, _ = eval_domains(model_mmd_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "        _, avg_acc_ARMCML_tmp, _ = test(algorithm, test_loader, n_samples_per_group = s)\n",
    "\n",
    "        acc_umaDANN_samples.append(avg_acc_umaDANN_tmp[-1])\n",
    "        acc_umaMMD_samples.append(avg_acc_umaMMD_tmp[-1])\n",
    "        acc_scratchDANN_samples.append(avg_acc_scratchDANN_tmp[-1])\n",
    "        acc_scratchMMD_samples.append(avg_acc_scratchMMD_tmp[-1])\n",
    "        acc_ARMCML_samples.append(avg_acc_ARMCML_tmp)\n",
    "        \n",
    "    write_in_file(acc_umaDANN_samples, PATH+\"acc_umaDANN_samples\")\n",
    "    write_in_file(acc_umaMMD_samples, PATH+\"acc_umaMMD_samples\")\n",
    "    write_in_file(acc_scratchDANN_samples, PATH+\"acc_scratchDANN_samples\")\n",
    "    write_in_file(acc_scratchMMD_samples, PATH+\"acc_scratchMMD_samples\")\n",
    "    write_in_file(acc_ARMCML_samples, PATH+\"acc_ARMCML_samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be8e8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"AVERAGE ACCURACY\")\n",
    "print(df_average_mean)\n",
    "#print(df_average_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78d8f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"WORST-CASE ACCURACY\")\n",
    "print(df_worse_mean)\n",
    "#print(df_worse_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f784c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Plot\n",
    "models_list = [\"perf_umaDANN\", \"perf_umaMMD\", \"perf_scratchDANN\", \"perf_scratchMMD\"]\n",
    "average_accuracy = []\n",
    "std_error = []\n",
    "for perf in models_list:\n",
    "    avg_acc = []\n",
    "    for i in range(3):\n",
    "        file_name = \"./\"+dataset_type+\"/Results\"+str(i+1)+\"/\"+perf\n",
    "        avg_acc.append(list(pd.read_pickle(file_name)['average accuracy']))\n",
    "    average_accuracy.append(np.mean(np.array(avg_acc),0))\n",
    "    std_error.append(np.std(np.array(avg_acc),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98751718",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(params.test_steps), average_accuracy[0], c='tab:blue', label=\"UMA-DANN\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[0]-std_error[0], average_accuracy[0]+std_error[0], color='tab:blue', alpha=0.2)\n",
    "plt.plot(range(params.test_steps), average_accuracy[1], c='tab:green', label=\"UMA-MMD\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[1]-std_error[1], average_accuracy[1]+std_error[1], color='tab:green', alpha=0.2)\n",
    "plt.plot(range(params.test_steps), average_accuracy[2], c='tab:orange', label=\"Scratch-DANN\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[2]-std_error[2], average_accuracy[2]+std_error[2], color='tab:orange', alpha=0.2)\n",
    "plt.plot(range(params.test_steps), average_accuracy[3], c='tab:purple', label=\"Scratch-MMD\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[3]-std_error[3], average_accuracy[3]+std_error[3], color='tab:purple', alpha=0.2)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"Average accuracy\", fontsize=12)\n",
    "plt.savefig(\"./\" + dataset_type + \"/average_accuracy_std.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d522d1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Comparison with ARM-CML\n",
    "models_list = [\"perf_ARM\", \"perf_umaDANN\", \"perf_umaMMD\"]\n",
    "\n",
    "df_mean = pd.DataFrame(columns=[\"Avg 200 steps\", \"Worse 200 steps\"], index=models_list)\n",
    "df_std = pd.DataFrame(columns=[\"Avg 200 steps\", \"Worse 200 steps\"], index=models_list)\n",
    "\n",
    "for perf in models_list:\n",
    "    df = pd.DataFrame(columns=[\"Avg 200 steps\", \"Worse 200 steps\"])\n",
    "    for i in range(3):\n",
    "        file_name = \"./\"+dataset_type+\"/Results\"+str(i+1)+\"/\"+perf\n",
    "        objects = pd.read_pickle(file_name)\n",
    "        if perf == \"perf_ARM\":\n",
    "            df.loc[len(df)] = [objects[\"average_acc\"], objects[\"worst_case_acc\"]]\n",
    "        else:\n",
    "            df.loc[len(df)] = [objects[\"average accuracy\"][-1], objects[\"worst_case_accuracy\"][-1]]\n",
    "    df_mean.loc[perf]=[df[\"Avg 200 steps\"].mean(), df[\"Worse 200 steps\"].mean()]\n",
    "    df_std.loc[perf]=[df[\"Avg 200 steps\"].std(), df[\"Worse 200 steps\"].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce6140",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"AVERAGE ACCURACY\")\n",
    "print(df_mean)\n",
    "#print(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2452c8a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Vary number of unlabeled samples at test time\n",
    "models_list = [\"acc_scratchDANN_samples\", \"acc_scratchMMD_samples\", \"acc_ARMCML_samples\", \"acc_umaDANN_samples\", \"acc_umaMMD_samples\"]\n",
    "\n",
    "df_mean = pd.DataFrame(columns=[\"50 examples\", \"100 examples\", \"150 examples\"], index=models_list)\n",
    "df_std = pd.DataFrame(columns=[\"50 examples\", \"100 examples\", \"150 examples\"], index=models_list)\n",
    "\n",
    "for perf in models_list:\n",
    "    df = pd.DataFrame(columns=[\"50 examples\", \"100 examples\", \"150 examples\"])\n",
    "    for i in range(3):\n",
    "        file_name = \"./\"+dataset_type+\"/Results\"+str(i+1)+\"/\"+perf\n",
    "        objects = pd.read_pickle(file_name)\n",
    "        df.loc[len(df)] = [objects[0], objects[1], objects[2]]\n",
    "    df_mean.loc[perf]=[df[\"50 examples\"].mean(), df[\"100 examples\"].mean(), df[\"150 examples\"].mean()]\n",
    "    df_std.loc[perf]=[df[\"50 examples\"].std(), df[\"100 examples\"].std(), df[\"150 examples\"].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7df67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"AVERAGE ACCURACY\")\n",
    "print(df_mean)\n",
    "#print(df_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
