{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbcaff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "def warn(*args, **kwargs): pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140e220d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/a/annvet/.conda/envs/ptorch/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c75b3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch, random, numpy as np, matplotlib.pyplot as plt, os, pickle, pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43adb48e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.system('nvidia-smi -q -d Memory |grep -A6 GPU|grep Free >tmp')\n",
    "memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "gpu_number =  int(np.argmax(memory_available))\n",
    "torch.cuda.set_device(gpu_number)\n",
    "print(f\"Cuda:{gpu_number}\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set dataset and parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "8abb4634",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set dataset and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fafcd28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_type = \"DailySports\" #RotatedMNIST, FEMNIST, CIFAR10C, DailySports, WISDM\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(1, \"./\" + dataset_type)\n",
    "import data, params, models\n",
    "from utils import write_in_file, plot_avg_acc, plot_worst_acc\n",
    "from trainingUMA import UMA, adapt_evaluate_dann, adapt_evaluate_mmd, eval_domains\n",
    "from trainingARM import ARM_CML, train, test\n",
    "\n",
    "# For reproducibility\n",
    "torch.random.manual_seed(params.seed)\n",
    "random.seed(params.seed)\n",
    "np.random.seed(params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9423d54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET\n",
      "Number of examples 62732\n",
      "Number of classes 62\n",
      "Number of domains 262\n",
      "Smallest domain:  104\n",
      "Largest domain:  443\n",
      "Number of examples 8439\n",
      "Number of classes 62\n",
      "Number of domains 35\n",
      "Smallest domain:  140\n",
      "Largest domain:  420\n",
      "MODELS-UMA\n",
      "MODELS-SCRATCH\n",
      "MODELS-ARM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/annvet/UMA_code/trainingARM.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X.append(torch.tensor(x)); Y.append(y); G.append(g)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/annvet/.conda/envs/ptorch/lib/python3.9/site-packages/numpy/core/_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3408648/1298543971.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     89\u001B[0m     \u001B[0mworst_acc_umaDANN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mavg_acc_umaDANN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtestacc_umaDANN\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval_domains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muma_dann\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_dann\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_cc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_dann\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_dd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msteps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_steps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malpha_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'dann'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_norm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_comparison\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_comparison\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mPATH\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\"perf_umaDANN\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[0mworst_acc_scratchDANN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mavg_acc_scratchDANN\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtestacc_scratchDANN\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval_domains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_dann_scratch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_dann\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_cc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_dann\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_dd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msteps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_steps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malpha_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'dann'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_norm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_comparison\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_comparison\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mPATH\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\"perf_scratchDANN\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 91\u001B[0;31m     \u001B[0mworst_acc_umaMMD\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mavg_acc_umaMMD\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtestacc_umaMMD\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval_domains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muma_mmd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_mmd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_cc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_mmd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_dd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msteps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_steps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msigma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msigma_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'mmd'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_norm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_comparison\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_comparison\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mPATH\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\"perf_umaMMD\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     92\u001B[0m     \u001B[0mworst_acc_scratchMMD\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mavg_acc_scratchMMD\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtestacc_scratchMMD\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval_domains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_mmd_scratch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdomains_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_mmd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_cc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muma_mmd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_sgd_dd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msteps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_steps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msigma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msigma_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'mmd'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_norm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_norm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_comparison\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_comparison\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_output\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mPATH\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m\"perf_scratchMMD\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/nfs/home/annvet/UMA_code/trainingUMA.py\u001B[0m in \u001B[0;36meval_domains\u001B[0;34m(model, domains_train, domains_test, loss_fn, lr_sgd_cc, lr_sgd_dd, steps, alpha, sigma, model_type, batch_norm, num_comparison, test_size, seed, save_output)\u001B[0m\n\u001B[1;32m    332\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mmodel_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'dann'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mhistory\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madapt_evaluate_dann\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr_sgd_cc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr_sgd_dd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mXs_sp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mys_sp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mXt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msteps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_norm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_norm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 334\u001B[0;31m                 \u001B[0;32melif\u001B[0m \u001B[0mmodel_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'mmd'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mhistory\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madapt_evaluate_mmd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr_sgd_cc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mXs_sp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mys_sp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mXt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msteps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msigma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_norm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_norm\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    335\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    336\u001B[0m             \u001B[0maccuracies\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"accuracy_tgt\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/nfs/home/annvet/UMA_code/trainingUMA.py\u001B[0m in \u001B[0;36madapt_evaluate_mmd\u001B[0;34m(model, loss_fn, lr_sgd_cc, Xs, ys, Xt, yt, steps, sigma, domain_eval_s, domain_eval_t, batch_norm, print_output)\u001B[0m\n\u001B[1;32m    268\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_classifier\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mloss_domain\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/ptorch/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 307\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/ptorch/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 154\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for it in range(3):\n",
    "    clear_output(wait=True)\n",
    "    PATH = \"./\" + dataset_type + \"/Results\" + str(it+1) + \"/\"\n",
    "    if not os.path.exists(PATH): os.makedirs(PATH)\n",
    "    print(\"DATASET\") \n",
    "    if dataset_type == \"RotatedMNIST\":\n",
    "        domains_train_idxs = random.sample(range(params.nb_domains_tot), params.nb_domains_train)\n",
    "        domains_test_idxs = list(set(range(params.nb_domains_tot)) - set(domains_train_idxs))\n",
    "        #domains_train_idxs = pd.read_pickle(PATH+\"domains_train\")\n",
    "        #domains_test_idxs = pd.read_pickle(PATH+\"domains_test\")\n",
    "        rotatedMnist = data.RotatedMNIST(print_output=False)\n",
    "        domains_train, domains_test = data.domain_generator(rotatedMnist, domains_train_idxs, domains_test_idxs)\n",
    "        dataset_train = data.RotatedMNIST(domains_idxs = domains_train_idxs, split='train', print_output=False)\n",
    "        dataset_test = data.RotatedMNIST(domains_idxs = domains_test_idxs, split='test', print_output=False)    \n",
    "        write_in_file(domains_train_idxs, PATH+\"domains_train\")\n",
    "        write_in_file(domains_test_idxs, PATH+\"domains_test\")\n",
    "    elif dataset_type == \"FEMNIST\":\n",
    "        dataset_train = data.FEMNISTDataset(split=\"train\")\n",
    "        domains_train = data.domain_generator(dataset_train, split=\"train\")\n",
    "        dataset_test = data.FEMNISTDataset(split=\"test\")\n",
    "        domains_test = data.domain_generator(dataset_test)\n",
    "    elif dataset_type == \"CIFAR10C\":\n",
    "        dataset_train = data.CIFARDataset(split='train')\n",
    "        domains_train = data.domain_generator(dataset_train)\n",
    "        dataset_test = data.CIFARDataset(split='test')\n",
    "        domains_test = data.domain_generator(dataset_test, test='True')\n",
    "    elif dataset_type == \"DailySports\":\n",
    "        root_dir = \"DailySports/\"\n",
    "        #domains_train_idxs = random.sample(range(params.nb_domains_tot), params.nb_domains_train)\n",
    "        #domains_test_idxs = list(set(range(params.nb_domains_tot)) - set(domains_train_idxs))\n",
    "        domains_train_idxs = pd.read_pickle(PATH+\"domains_train\")\n",
    "        domains_test_idxs = pd.read_pickle(PATH+\"domains_test\")\n",
    "        HAR_Dataset = data.HARDataset(root_dir, feature_reduction=params.feature_reduction, extract_features=params.extract_features)\n",
    "        domains_train, domains_test = data.domain_generator(HAR_Dataset, domains_train_idxs, domains_test_idxs)\n",
    "        dataset_train = data.HARDataset(root_dir, domains_train_idxs, feature_reduction=params.feature_reduction, extract_features=params.extract_features)\n",
    "        dataset_test = data.HARDataset(root_dir, domains_test_idxs, feature_reduction=params.feature_reduction, extract_features=params.extract_features)\n",
    "    elif dataset_type == \"WISDM\":  \n",
    "        root_dir = \"WISDM/\"\n",
    "        domains_total = list(range(params.nb_domains_tot))\n",
    "        #Remove subjects with corrupted data\n",
    "        domains_total.remove(14)\n",
    "        domains_total.remove(18)\n",
    "        domains_total.remove(42)\n",
    "        domains_train_idxs = random.sample(domains_total, params.nb_domains_train)\n",
    "        domains_test_idxs = list(set(domains_total)-set(domains_train_idxs))\n",
    "        WISDM_Dataset = data.WISDMDataset(root_dir, domains_total, datatype = params.data_type, extract_features=params.extract_features)\n",
    "        domains_train, domains_test = data.domain_generator(WISDM_Dataset, domains_train_idxs, domains_test_idxs)\n",
    "        dataset_train = data.WISDMDataset(root_dir, domains_train_idxs, datatype=params.data_type, extract_features=params.extract_features)\n",
    "        dataset_test = data.WISDMDataset(root_dir, domains_test_idxs, datatype=params.data_type, extract_features=params.extract_features)\n",
    "        \n",
    "    train_loader = data.get_loader(dataset_train, sampler_type='group', uniform_over_groups=1,\n",
    "                              meta_batch_size=params.meta_batch_size,\n",
    "                              support_size=params.support_size,\n",
    "                              shuffle=True,\n",
    "                              pin_memory=True, num_workers=8)\n",
    "    test_loader = data.get_loader(dataset_test, sampler_type='group', uniform_over_groups=False,\n",
    "                              meta_batch_size=params.meta_batch_size,\n",
    "                              support_size=params.support_size,\n",
    "                              shuffle=False,\n",
    "                              pin_memory=True, num_workers=8)  \n",
    "\n",
    "    print(\"MODELS-UMA\")\n",
    "    #UMA models\n",
    "    model_dann = models.DANNModel().to(DEVICE)\n",
    "    uma_dann = UMA(model_dann, params.loss.to(DEVICE), params.lr_sgd_cc, params.lr_sgd_dd, params.lr_adam, adapt_steps=params.ADAPT_STEPS,  model_type = 'dann', adaptive_lr=True, batch_norm=params.batch_norm).fit_dann(domains_train, alpha=params.alpha_, steps=params.training_steps)\n",
    "    torch.save(uma_dann, PATH+\"model_dann.pkl\")\n",
    "    #uma_dann = torch.load(PATH+\"model_dann.pkl\").to(DEVICE)\n",
    "\n",
    "    model_mmd = models.MMDModel().to(DEVICE)\n",
    "    uma_mmd = UMA(model_mmd, params.loss.to(DEVICE), params.lr_sgd_cc, params.lr_sgd_dd, params.lr_adam, adapt_steps=params.ADAPT_STEPS, model_type = 'mmd', adaptive_lr=True, sigma = params.sigma_, batch_norm=params.batch_norm).fit_mmd(domains_train, steps=params.training_steps)\n",
    "    torch.save(uma_mmd, PATH+\"model_mmd.pkl\")\n",
    "    #uma_mmd = torch.load(PATH+\"model_mmd.pkl\").to(DEVICE)\n",
    "    \n",
    "    print(\"MODELS-SCRATCH\")\n",
    "    #models from scratch\n",
    "    model_dann_scratch = models.DANNModel().to(DEVICE) \n",
    "    model_mmd_scratch = models.MMDModel().to(DEVICE)\n",
    "\n",
    "    print(\"MODELS-ARM\")\n",
    "    # ARM CML\n",
    "    context_net = models.ContextNet(in_dim=params.input_dim, out_dim=params.output_dim, hidden_dim=params.hidden_dim_context).to(DEVICE)\n",
    "    model_arm = models.ARMNet(in_dim=params.input_dim_arm, hidden_dim=params.hidden_dim_net, n_classes=params.n_classes).to(DEVICE)\n",
    "    algorithm = ARM_CML(model_arm, context_net, params.input_dim, params.loss.to(DEVICE), learning_rate=params.learning_rate, weight_decay=params.weight_decay, optimizer=params.optimizer, momentum=params.momentum, support_size=params.support_size, device=DEVICE, adapt_bn=params.adapt_bn, img_dataset=params.img_dataset)\n",
    "    history_arm = train(algorithm, train_loader, test_loader, epochs = params.epochsARM)\n",
    "    torch.save(algorithm, PATH+\"model_arm.pkl\")\n",
    "    algorithm = torch.load(PATH+\"model_arm.pkl\").to(DEVICE)\n",
    "    \n",
    "    print(\"PERFORMANCE\")\n",
    "    worst_acc_umaDANN, avg_acc_umaDANN, testacc_umaDANN = eval_domains(uma_dann.model, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_umaDANN\")\n",
    "    worst_acc_scratchDANN, avg_acc_scratchDANN, testacc_scratchDANN = eval_domains(model_dann_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_scratchDANN\")\n",
    "    worst_acc_umaMMD, avg_acc_umaMMD, testacc_umaMMD = eval_domains(uma_mmd.model, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_umaMMD\")\n",
    "    worst_acc_scratchMMD, avg_acc_scratchMMD, testacc_scratchMMD = eval_domains(model_mmd_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, num_comparison=params.num_comparison, save_output=PATH+\"perf_scratchMMD\")\n",
    "\n",
    "    plot_avg_acc(params.test_steps, avg_acc_umaDANN, avg_acc_umaMMD, avg_acc_scratchDANN, avg_acc_scratchMMD, save_file=PATH+\"average_accuracy.png\")\n",
    "    plot_worst_acc(params.test_steps, worst_acc_umaDANN, worst_acc_umaMMD, worst_acc_scratchDANN, worst_acc_scratchMMD, save_file=PATH+\"worst_accuracy.png\")\n",
    "\n",
    "    worst_case_acc, average_case_acc, stats = test(algorithm, test_loader, n_samples_per_group=params.n_samples_test, save_output=PATH+\"perf_ARM\")\n",
    "\n",
    "    print(\"#SAMPLES COMPARISON\")\n",
    "    n_samples = [50, 100, 145]\n",
    "    acc_umaDANN_samples, acc_umaMMD_samples, acc_scratchDANN_samples, acc_scratchMMD_samples, acc_ARMCML_samples=[], [], [], [], []\n",
    "    for s in n_samples:\n",
    "        print(s)\n",
    "        seed_ = random.randint(0, 100)\n",
    "        _, avg_acc_umaDANN_tmp, _ = eval_domains(uma_dann.model, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "        _, avg_acc_umaMMD_tmp, _ = eval_domains(uma_mmd.model, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "\n",
    "        _, avg_acc_scratchDANN_tmp, _ = eval_domains(model_dann_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_dann.lr_sgd_cc, uma_dann.lr_sgd_dd, steps = params.test_steps, alpha = params.alpha_, model_type='dann', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "        _, avg_acc_scratchMMD_tmp, _ = eval_domains(model_mmd_scratch, domains_train, domains_test, params.loss.to(DEVICE), uma_mmd.lr_sgd_cc, uma_mmd.lr_sgd_dd, steps = params.test_steps, sigma=params.sigma_, model_type='mmd', batch_norm=params.batch_norm, test_size = s, seed = seed_)\n",
    "        _, avg_acc_ARMCML_tmp, _ = test(algorithm, test_loader, n_samples_per_group = s)\n",
    "\n",
    "        acc_umaDANN_samples.append(avg_acc_umaDANN_tmp[-1])\n",
    "        acc_umaMMD_samples.append(avg_acc_umaMMD_tmp[-1])\n",
    "        acc_scratchDANN_samples.append(avg_acc_scratchDANN_tmp[-1])\n",
    "        acc_scratchMMD_samples.append(avg_acc_scratchMMD_tmp[-1])\n",
    "        acc_ARMCML_samples.append(avg_acc_ARMCML_tmp)\n",
    "        \n",
    "    write_in_file(acc_umaDANN_samples, PATH+\"acc_umaDANN_samples\")\n",
    "    write_in_file(acc_umaMMD_samples, PATH+\"acc_umaMMD_samples\")\n",
    "    write_in_file(acc_scratchDANN_samples, PATH+\"acc_scratchDANN_samples\")\n",
    "    write_in_file(acc_scratchMMD_samples, PATH+\"acc_scratchMMD_samples\")\n",
    "    write_in_file(acc_ARMCML_samples, PATH+\"acc_ARMCML_samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42799b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ef41e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Comparison with traditional UDA methods\n",
    "models_list = [\"perf_scratchDANN\", \"perf_scratchMMD\", \"perf_umaDANN\", \"perf_umaMMD\"]\n",
    "\n",
    "df_average_mean = pd.DataFrame(columns=[\"1step\", \"5steps\", \"200steps\"], index=models_list)\n",
    "df_worse_mean = pd.DataFrame(columns=[\"1step\", \"5steps\", \"200steps\"], index=models_list)\n",
    "df_average_std = pd.DataFrame(columns=[\"1step\", \"5steps\", \"200steps\"], index=models_list)\n",
    "df_worse_std = pd.DataFrame(columns=[\"1step\", \"5steps\", \"200steps\"], index=models_list)\n",
    "\n",
    "for perf in models_list:\n",
    "    df_average = pd.DataFrame(columns=[\"1step\", \"5steps\", \"200steps\"])\n",
    "    df_worse = pd.DataFrame(columns=[\"1step\", \"5steps\", \"200steps\"])\n",
    "    for i in range(3):\n",
    "        file_name = \"./\"+dataset_type+\"/Results\"+str(i+1)+\"/\"+perf\n",
    "        objects = pd.read_pickle(file_name)\n",
    "        df_average.loc[len(df_average)] = [objects[\"average accuracy\"][1], objects[\"average accuracy\"][5], objects[\"average accuracy\"][-1]]\n",
    "        df_worse.loc[len(df_worse)] = [objects[\"worst_case_accuracy\"][1], objects[\"worst_case_accuracy\"][5], objects[\"worst_case_accuracy\"][-1]]\n",
    "    df_average_mean.loc[perf]=[df_average[\"1step\"].mean(), df_average[\"5steps\"].mean(), df_average[\"200steps\"].mean()]\n",
    "    df_average_std.loc[perf]=[df_average[\"1step\"].std(), df_average[\"5steps\"].std(), df_average[\"200steps\"].std()]\n",
    "    \n",
    "    df_worse_mean.loc[perf]=[df_worse[\"1step\"].mean(), df_worse[\"5steps\"].mean(), df_worse[\"200steps\"].mean()]\n",
    "    df_worse_std.loc[perf]=[df_worse[\"1step\"].std(), df_worse[\"5steps\"].std(), df_worse[\"200steps\"].std()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be8e8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"AVERAGE ACCURACY\")\n",
    "print(df_average_mean)\n",
    "#print(df_average_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78d8f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"WORST-CASE ACCURACY\")\n",
    "print(df_worse_mean)\n",
    "#print(df_worse_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f784c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Plot\n",
    "models_list = [\"perf_umaDANN\", \"perf_umaMMD\", \"perf_scratchDANN\", \"perf_scratchMMD\"]\n",
    "average_accuracy = []\n",
    "std_error = []\n",
    "for perf in models_list:\n",
    "    avg_acc = []\n",
    "    for i in range(3):\n",
    "        file_name = \"./\"+dataset_type+\"/Results\"+str(i+1)+\"/\"+perf\n",
    "        avg_acc.append(list(pd.read_pickle(file_name)['average accuracy']))\n",
    "    average_accuracy.append(np.mean(np.array(avg_acc),0))\n",
    "    std_error.append(np.std(np.array(avg_acc),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98751718",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(params.test_steps), average_accuracy[0], c='tab:blue', label=\"UMA-DANN\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[0]-std_error[0], average_accuracy[0]+std_error[0], color='tab:blue', alpha=0.2)\n",
    "plt.plot(range(params.test_steps), average_accuracy[1], c='tab:green', label=\"UMA-MMD\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[1]-std_error[1], average_accuracy[1]+std_error[1], color='tab:green', alpha=0.2)\n",
    "plt.plot(range(params.test_steps), average_accuracy[2], c='tab:orange', label=\"Scratch-DANN\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[2]-std_error[2], average_accuracy[2]+std_error[2], color='tab:orange', alpha=0.2)\n",
    "plt.plot(range(params.test_steps), average_accuracy[3], c='tab:purple', label=\"Scratch-MMD\")\n",
    "plt.fill_between(range(params.test_steps), average_accuracy[3]-std_error[3], average_accuracy[3]+std_error[3], color='tab:purple', alpha=0.2)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.legend(loc=4)\n",
    "plt.title(\"Average accuracy\", fontsize=12)\n",
    "plt.savefig(\"./\" + dataset_type + \"/average_accuracy_std.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d522d1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Comparison with ARM-CML\n",
    "models_list = [\"perf_ARM\", \"perf_umaDANN\", \"perf_umaMMD\"]\n",
    "\n",
    "df_mean = pd.DataFrame(columns=[\"Avg 200 steps\", \"Worse 200 steps\"], index=models_list)\n",
    "df_std = pd.DataFrame(columns=[\"Avg 200 steps\", \"Worse 200 steps\"], index=models_list)\n",
    "\n",
    "for perf in models_list:\n",
    "    df = pd.DataFrame(columns=[\"Avg 200 steps\", \"Worse 200 steps\"])\n",
    "    for i in range(3):\n",
    "        file_name = \"./\"+dataset_type+\"/Results\"+str(i+1)+\"/\"+perf\n",
    "        objects = pd.read_pickle(file_name)\n",
    "        if perf == \"perf_ARM\":\n",
    "            df.loc[len(df)] = [objects[\"average_acc\"], objects[\"worst_case_acc\"]]\n",
    "        else:\n",
    "            df.loc[len(df)] = [objects[\"average accuracy\"][-1], objects[\"worst_case_accuracy\"][-1]]\n",
    "    df_mean.loc[perf]=[df[\"Avg 200 steps\"].mean(), df[\"Worse 200 steps\"].mean()]\n",
    "    df_std.loc[perf]=[df[\"Avg 200 steps\"].std(), df[\"Worse 200 steps\"].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce6140",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"AVERAGE ACCURACY\")\n",
    "print(df_mean)\n",
    "#print(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2452c8a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Vary number of unlabeled samples at test time\n",
    "models_list = [\"acc_scratchDANN_samples\", \"acc_scratchMMD_samples\", \"acc_ARMCML_samples\", \"acc_umaDANN_samples\", \"acc_umaMMD_samples\"]\n",
    "\n",
    "df_mean = pd.DataFrame(columns=[\"50 examples\", \"100 examples\", \"150 examples\"], index=models_list)\n",
    "df_std = pd.DataFrame(columns=[\"50 examples\", \"100 examples\", \"150 examples\"], index=models_list)\n",
    "\n",
    "for perf in models_list:\n",
    "    df = pd.DataFrame(columns=[\"50 examples\", \"100 examples\", \"150 examples\"])\n",
    "    for i in range(3):\n",
    "        file_name = \"./\"+dataset_type+\"/Results\"+str(i+1)+\"/\"+perf\n",
    "        objects = pd.read_pickle(file_name)\n",
    "        df.loc[len(df)] = [objects[0], objects[1], objects[2]]\n",
    "    df_mean.loc[perf]=[df[\"50 examples\"].mean(), df[\"100 examples\"].mean(), df[\"150 examples\"].mean()]\n",
    "    df_std.loc[perf]=[df[\"50 examples\"].std(), df[\"100 examples\"].std(), df[\"150 examples\"].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7df67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"AVERAGE ACCURACY\")\n",
    "print(df_mean)\n",
    "#print(df_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}